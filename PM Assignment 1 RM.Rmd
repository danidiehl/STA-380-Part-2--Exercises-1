---
title: "PM Assignment 1"
author: "Dani Diehl, Gihani D, Chloe Kwon, Jess Chung"
date: "August 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Probability Practice 
### Part A

![Part A](/Users/danid/Pictures/ProbPracticePartA.jpg)

As shown by the math above, the fraction of people who are truthful clickers and answered "Yes" is 5/7. 
### Part B

![Part A](/Users/danid/Pictures/ProbPracticePartB.jpg)

As shown by the math above, the probability of having the disease after testing positive is 19.9%. This means the test is not a particularly good indicator of having the disease and that the test is giving a lot of False Positives. If a universal testing policy were implemented, additional screening and testing would need to be done on those who tested positive to further confirm whether or not they have the disease. 

## Exploratory Analysis: green buildings

```{r}
gb<-read.csv("greenbuildings.csv")
gb[is.na(gb)] <- 0
```
Using exploratory analysis, we can see that the analysis performed by on-staff stats guru is flawed in the following ways:
1. Filtering out buildings with low occupancy rates effectively removed 214 non-green buildings and 1 green building from the dataset which potentially could result in biased results. 
```{r}
gb.scrubbed <- gb[gb$leasing_rate<10,]
cat("Number of buildings removed by analyst:",nrow(gb.scrubbed),"\n")
cat("Number of non-green buildings removed by analyst:",nrow(gb.scrubbed[gb.scrubbed$green_rating==0,]))
```
The analyst's logic that buildings with less than 10% occupancy are "weird" and should be subject to removal is incomplete. Additional analysis needs to be performed to determine the cause behind these low occupancy rates before this data can be dismissed. 
```{r}
gb.mean = apply(gb, 2, mean)
gb.scrubbed.mean = apply(gb.scrubbed, 2, mean)
gb.scrubbed.mean - gb.mean
```
On average, we can see that the removed buildings have on average lower rent, a higher age and are less likely to be close to amenities or be in the nicest Class A of buildings. Higher age and lack of amenities may explain low occupancy rates. For the purposes of our analysis, we will include these low occupancy buildings. 

2.Looking at green buildings and non-green buildings and their relationship with rent without accounting for other variables does show that the green rating variable is significant. Because the coefficient is positive, we might assume that green buildings have higher rents.

```{r}
basic<-lm(gb$Rent~gb$green_rating)
summary(basic)
```
However, we must control for other variables that may be the true reason behind the underlying effect. 

```{r}
summary(lm(Rent~.,data=gb))
```

When looking at the output of the regression of Rent on all of the variables in the dataset, green rating does not appear to be significant. This means that holding all other variables constant, green rating may not have an impact on the rent charged. From this output, other variables, including electricity and gas costs, the class of the building, the size, and the employment growth rate of the region, seem to be more important in predicting the rent.
 
```{r}
boxplot(gb$Rent~gb$green_rating, pch=19, col='pink',xlab='Green Rating',ylab='Rent')
```
Green buildings have a slightly higher median rent than non-green buildings. However, the range of the rent of green buildings is much smaller than the range of rent of non-green buildings. Non-green buildings seem to have more outliers with very high rent. We must dig deeper into other factors that could be contributing to the difference in rent.

```{r}
library(ggplot2)
t<-ggplot(gb,aes(x=cluster, y=Rent))
t+geom_point(aes(color=factor(green_rating)))
```
By graphing cluster on the x-axis and Rent on the y-axis, we attempt to control for other factors that may be influencing the rent. Clusters were made based on buildings located in the same geographical region. As you can see from the graph, the green buildings (shown in light blue) are not near the top of the rent for each cluster. Green buildings may have higher rent on average, but when limited to clusters, they do not appear to have the highest rent.

3. Since whether or not a building is green is not a strong predictor of rent, the analyst's conclusion that the developer could receive an extra $650K a year is incorrect. Further more, the analyst's assertion that the developer could continue to earn an extra $650K for the lifetime of the building is incorrect. From the multiple regression above, we can see that an increase in age of the building results in a statistically significant decrease in rent values. Also for the purposes of the diagram below, we can see that simple regression also shows that an increase in age results in a decrease in rent. 

```{r}
library(ggplot2)
ggplot(gb,aes(x=age, y=Rent))+geom_point(aes(color=factor(green_rating)))+ geom_smooth(method = 'lm', formula=y~x)
age.lm = lm(gb$Rent~gb$age)
summary(age.lm)
coef(age.lm)

```

4. Lastly, the assumption that the building would be able to recoup the cost of green certification after 8 years assuming 90% occupancy is potentially faulty. While the general distribution of leasing rates for green and non-green buildings are similar, this still means that 38% of green buildings have lower than 90% occupancy. As we have explained above, this low occupancy could be explained by other factors such as amenities and employment growth rate. However, without further examination of these values for Austin, we cannot make assumptions of greater than 90% occupancy.

```{r}
library(ggplot2)
ggplot(gb,aes(x=leasing_rate))+geom_histogram(breaks=seq(20, 100, by=5), aes(color=factor(green_rating)))+ facet_wrap(~green_rating)
cat("Number of green buildings:",count(gb$green_rating==1),"\n")
cat("Number of green buildings with less than 90% occupancy:",count(gb$leasing_rate<90 & gb$green_rating==1),"\n")
cat("Percentage of green buildings with less than 90% occupancy:", count(gb$leasing_rate<90 & gb$green_rating==1)/count(gb$green_rating==1))
```


In conclusion, there are several flaws in the analyst's report that primarily stem from an oversimplification of the problem at hand. With additional consideration to other variables such as age, growth in employment, neighborhood cluster etc, we observe that the green rating for a building is not a statistically significant predictor of rent/income to the developer. 

## Bootstrapping

1. Characterize risk/return properties of US domestic equities (SPY: the S&P 500 stock index), US Treasury bonds (TLT), Investment-grade corporate bonds (LQD), Emerging-market equities (EEM), and Real estate (VNQ).


```{r}
library(mosaic)
library(quantmod) #stock info mod
library(foreach)


mystocks = c("SPY", "TLT", "LQD","EEM","VNQ")
getSymbols(mystocks) #goes on yahoo finance, downloads data

for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

SPYa_returns = as.matrix(na.omit(ClCl(SPYa)))
SPYReturnAvg=mean(SPYa_returns)
SPYReturnSD=sd(SPYa_returns)
spy=c(SPYReturnAvg,SPYReturnSD)

TLTa_returns = as.matrix(na.omit(ClCl(TLTa)))
TLTReturnAvg=mean(TLTa_returns)
TLTReturnSD=sd(TLTa_returns)
tlt=c(TLTReturnAvg,TLTReturnSD)

LQDa_returns = as.matrix(na.omit(ClCl(LQDa)))
LQDReturnAvg=mean(LQDa_returns)
LQDReturnSD=sd(LQDa_returns)
lqd=c(LQDReturnAvg,LQDReturnSD)

EEMa_returns = as.matrix(na.omit(ClCl(EEMa)))
EEMReturnAvg=mean(EEMa_returns)
EEMReturnSD=sd(EEMa_returns)
eem=c(EEMReturnAvg,EEMReturnSD)

VNQa_returns = as.matrix(na.omit(ClCl(VNQa)))
VNQReturnAvg=mean(VNQa_returns)
VNQReturnSD=sd(VNQa_returns)
vnq=c(VNQReturnAvg,VNQReturnSD)

df<-data.frame(spy,tlt,lqd,eem,vnq)
rownames(df)<-c('Avg Return', 'SD Return')
df
```
As you can see from the above dataframe, the Emerging-market equities has the highest expected return, but also the highest standard deviation. Therefore, EEM is the riskiest of the above ETFs. The Investment-grade corporate bonds have the lowest average return and also the lowest standard deviation. These facts make LQD the safest choice as an investment, but you have a smaller chance of making a larger return. US domestic equities and US Treasury bonds (SPY and TLT) are also relatively safe investments, with low risk but relatively low returns. Real estate (VNQ) has the second highest average return and the second highest standard deviation as well, making it safer than EEM but riskier than the rest. 


2. Outline our choice of safe and aggressive portfolios. 

For our safe portfolio, we decided to only invest in Investment-grade corporate bonds (LQD), US treasury bonds (TLT), and US domestic equities (SPY). These were the three safest investment choices due to their low standard deviation and risk. The expected returns are not as high as the other ETF's, but in the safe portfolio, we minimize the risk in the risk-reward tradeoff. We invested 60% in LQD (lowest risk), 30% in TLT, and 10% in SPY.
For our aggressive portfolio, we decided to only invest in the Emerging-market equities (EEM) and Real estate (VNQ). These have the highest expected return and the highest standard deviation/risk. In this portfolio, we are increases the risk to hopefully increase the reward. We invested 80% in EEM (the riskiest) and 20% in VNQ.


3. Use bootstrap resampling to estimate the 4-week (20 trading day) value at risk of each of your three portfolios at the 5% level.

```{r}
#####################################################
#equal split
#####################################################
all_returns1 = cbind(	ClCl(SPYa),
                     ClCl(TLTa),
                     ClCl(LQDa),
                     ClCl(EEMa),
                     ClCl(VNQa))
all_returns1 = as.matrix(na.omit(all_returns1))

# Now simulate many different possible scenarios  
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns1, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today 
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    holdings = weights * total_wealth 
    holdingstracker[today] = holdings
  }
  wealthtracker
}
# Calculate 5% value at risk
equalVAR=quantile(sim1[,n_days], 0.05) - initial_wealth

################################################################
#something safer
################################################################

all_returns2 = cbind(	ClCl(SPYa),
                      ClCl(TLTa),
                      ClCl(LQDa))
all_returns2 = as.matrix(na.omit(all_returns2))

# Now simulate many different possible scenarios  
initial_wealth = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.1, 0.3, 0.6)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns2, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    holdings = weights * total_wealth 
  }
  wealthtracker
}
# Calculate 5% value at risk
safeVAR=quantile(sim2[,n_days], 0.05) - initial_wealth

################################################################
#something more aggressive
################################################################

all_returns3 = cbind(ClCl(EEMa),
                      ClCl(VNQa))
all_returns3 = as.matrix(na.omit(all_returns3))

# Now simulate many different possible scenarios  
initial_wealth = 100000
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.8,0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns3, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    holdings = weights * total_wealth 
  }
  wealthtracker
}
# Calculate 5% value at risk
aggressiveVAR=quantile(sim3[,n_days], 0.05) - initial_wealth
df2=rbind(equalVAR, safeVAR, aggressiveVAR)
df2

```
In the dataframe labeled df2, you can see the value at risk at the 5% level for the Equal portfolio, Safe portfolio, and Aggressive portfolio, respectively. As you would expect, the value at risk for the safest portfolio is the smallest, and the value at risk for the aggressive portfolio is the largest.

4. Compare the results for each portfolio in a way that would allow the reader to make an intelligent decision among the three options.

```{r}

sd1=sd(sim1[,n_days]) #5655.20
mean1=mean(sim1[,n_days]) #100907.62

equal_confint= range(mean1-2*sd1, mean1+2*sd1)

sd2=sd(sim2[,n_days]) #2277.48 (smaller than sd1--less range) 
mean2=mean(sim2[,n_days]) #100522 (less than mean1)
safe_confint= range(mean2-2*sd2,mean2+2*sd2)


sd3=sd(sim3[,n_days]) #14884 (much bigger variation) 
mean3=mean(sim3[,n_days]) #101533 (much more than others)
agg_confint=range(mean3-2*sd3, mean3+2*sd3)

df3<-rbind(equal_confint,safe_confint,agg_confint)
df3

```
In the dataframe above, you can see the 95% confidence intervals for the total wealth one is expected to have after investing $100,000 into the equal, safe, and aggressive portfolios for 20 trading days. The 95% confidence interval for the equal split porfolio is [89597, 112218]. The 95% confidence interval for the safe portfolio is [95967, 105077]. The 95% confidence interval for the aggressive portfolio is [71764,131301]. Readers will be able to look at this information, see the potential for both loss and gain in each portfolio, and make a decision based on these facts. By computing the expected gain, expected standard deviation, and 95% confidence interval, readers will have a good idea about the risk-reward tradeoff for each portfolio.


## Market Segmentation

```{r}
library(ggplot2)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(tidyr)

ms<-read.csv("social_marketing.csv", header=TRUE)
ms$X <- NULL
ms$spam <- NULL
ms$adult <- NULL
ms = ms/rowSums(ms)
X = scale(ms, center=TRUE, scale=TRUE)

k.max = 25
data <- X
sil <- rep(0, k.max)
# Compute the average silhouette width for k = 2 to k = 25
set.seed(1)

for(i in 2:k.max) {
  cat("computing cluster for centers =",i,"\n")
  km.res <- kmeanspp(data, k = i, nstart = 20)
  ss <- silhouette(km.res$cluster, dist(data))
  sil[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil, type = "b", pch = 19, 
     frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)
which.max(sil) #Max is 10

set.seed(1)
ms$cluster <- kmeanspp(X, k=which.max(sil), nstart=20)$cluster #using max of 10

#cluster 1, high chatter, shopping, photo sharing
shopping<- gather(data = ms, -shopping,  -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = shopping, aes(x = value, y = shopping, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#cluster 2, politics, computer, travel, business
politics <- gather(data = ms, -politics, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = politics, aes(x = value, y = politics, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#cluster 3, art and tv/film, current events, crafts
art<- gather(data = ms, -art, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = art, aes(x = value, y = art, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#cluster 4, high online gaming, sports playing, college
college <- gather(data = ms, -college_uni, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = college, aes(x = value, y = college_uni, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#Cluster 5, cooking, beauty, fashion, photo sharing
cooking <- gather(data = ms, -cooking, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = cooking, aes(x = value, y = cooking, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#cluster 6 hard to parse out

#Cluster 7 is Outdoors, Personal Fitness, Healthy
health <- gather(data = ms, -health_nutrition, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = health, aes(x = value, y = health_nutrition, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#cluster 8 is high auto, high news, 
automotive<- gather(data = ms, -automotive, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = automotive, aes(x = value, y = automotive, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#cluster 9, high dating, low everything else
date<- gather(data = ms, -dating, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = date, aes(x = value, y = dating, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")

#cluster 10, high family, parenting, food, religion, school, sports fandom
parenting <- gather(data = ms, -parenting, -adult, -spam, -uncategorized, -cluster, key = "var", value = "value")
ggplot(data = parenting, aes(x = value, y = parenting, color = factor(cluster))) + geom_point() + facet_wrap(~ var, scales = "free")


```
