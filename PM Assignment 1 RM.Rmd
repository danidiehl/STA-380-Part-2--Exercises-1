---
title: "PM Assignment 1"
author: "Dani Diehl, Gihani D, Chloe Kwon, Jess Chung"
date: "August 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Probability Practice 
### Part A

![Part A](/Users/danid/Pictures/ProbPracticePartA.jpg)

As shown by the math above, the fraction of people who are truthful clickers and answered "Yes" is 5/7. 
### Part B

![Part A](/Users/danid/Pictures/ProbPracticePartB.jpg)

As shown by the math above, the probability of having the disease after testing positive is 19.9%. This means the test is not a particularly good indicator of having the disease and that the test is giving a lot of False Positives. If a universal testing policy were implemented, additional screening and testing would need to be done on those who tested positive to further confirm whether or not they have the disease. 

## Exploratory Analysis: green buildings

```{r}
gb<-read.csv("greenbuildings.csv")
basic<-lm(gb$Rent~gb$green_rating)
summary(basic)

```
When looking at the output from the simple regression of Rent on green rating, the green rating variable appears to be significant. Because the coefficient is positive, we might assume that green buildings have higher rents. However, we must control for other variables that may be the true reason behind the underlying effect. 

```{r}
summary(lm(Rent~.,data=gb))
```
When looking at the output of the regression of Rent on all of the variables in the dataset, green rating does not appear to be significant. This means that holding all other variables constant, green rating may not have an impact on the rent charged. From this output, other variables, including electricity and gas costs, the class of the building, the size, and the employment growth rate of the region, seem to be more important in predicting the rent.


```{r}
boxplot(gb$Rent~gb$green_rating, pch=19, col='pink',xlab='Green Rating',ylab='Rent')
```

```{r}
boxplot(gb$leasing_rate~gb$green_rating, col="gold",xlab='Green Rating',ylab='Leasing Rate')
```
Green buildings have a higher median leasing rate and less low leasing rate outliers. This could mean that a higher percentage of green buildings are leased out to tenants, perhaps due to people wanting to be environmentally friendly or the perception that more work gets done in naturally lit facilities. 
```{r}
library(ggplot2)
t<-ggplot(gb,aes(x=cluster, y=Rent))
t+geom_point(aes(color=green_rating))
```
By graphing cluster on the x-axis and Rent on the y-axis, we attempt to control for other factors that may be influencing the rent. Clusters were made based on buildings located in the same geographical region. As you can see from the graph, the green buildings (shown in light blue) are not near the top of the rent for each cluster. Green buildings may have higher rent on average, but when limited to clusters, they do not appear to have the highest rent.
```{r}
library(ggplot2)

ggplot(gb,aes(x=age, y=Rent))+geom_point(aes(color=green_rating)) + facet_wrap(~green_rating)

```
Green buildings exhibit the same decline in rent with increase in age.

```{r}
gb.scrubbed <- gb[gb$leasing_rate<10,]
nrow(gb.scrubbed)
nrow(gb.scrubbed[gb.scrubbed$green_rating==0,])

gb.missingscrubbed <- gb[gb$leasing_rate>10,]
basic.missingscrubbed <-lm(gb.missingscrubbed$Rent~gb.missingscrubbed$green_rating)
summary(basic.missingscrubbed)
summary(lm(Rent~.,data=gb.missingscrubbed))

```
When performing the same analysis on the scrubbed data that the analyst in the scenario used (ie Leasing Rate > 10%), we see the same results as above. Of the 215 buildings that had less than 10% occupancy, only one of them is a green building. Removing these buildings could have distorted the analysis and biased the data. 
```{r}
factorclass <- ifelse(gb$class_a==0, "class b", "class a")
gb$factorclass = factorclass

ggplot(gb,aes(x=age, y=Rent))+geom_point(aes(color=green_rating)) + facet_wrap(~factorclass)
ggplot(gb,aes(Rent)) + geom_histogram() + facet_wrap(~green_rating)
```


Through the above exploratory analysis, we can see that the analysis performed by on-staff stats guru is flawed in the following ways:
* Filtering out buildings with low occupancy rates effectively removed 214 non-green buildings and 1 green building, potentially biasing the data.
*Looking at green buildings and non-green buildings and their relationship with rent does not account for the other variables that may be truly predicting the relationship between a given building and its rent. As shown in the multiple linear regression and the plot of cluster and rent above, there are many other predictors such as Class of building and Cluster that are statistically siginificant predictors of rent per square foot. 
*Since whether or not a building is green is not a strong predictor of rent, the analyst's conclusion that the developer could receive an extra $650K a year is incorrect. Further more, the analyst's assertion that the developer could continue to earn an extra $650K for the lifetime of the building is incorrect. From the multiple regression above, we can see that an increase in age of the building results in a statistically significant decrease in rent values. 


## Bootstrapping

1. Characterize risk/return properties of US domestic equities (SPY: the S&P 500 stock index), US Treasury bonds (TLT), Investment-grade corporate bonds (LQD), Emerging-market equities (EEM), and Real estate (VNQ).


```{r}
library(mosaic)
library(quantmod) #stock info mod
library(foreach)


mystocks = c("SPY", "TLT", "LQD","EEM","VNQ")
getSymbols(mystocks) #goes on yahoo finance, downloads data

for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

SPYa_returns = as.matrix(na.omit(ClCl(SPYa)))
SPYReturnAvg=mean(SPYa_returns)
SPYReturnSD=sd(SPYa_returns)
spy=c(SPYReturnAvg,SPYReturnSD)

TLTa_returns = as.matrix(na.omit(ClCl(TLTa)))
TLTReturnAvg=mean(TLTa_returns)
TLTReturnSD=sd(TLTa_returns)
tlt=c(TLTReturnAvg,TLTReturnSD)

LQDa_returns = as.matrix(na.omit(ClCl(LQDa)))
LQDReturnAvg=mean(LQDa_returns)
LQDReturnSD=sd(LQDa_returns)
lqd=c(LQDReturnAvg,LQDReturnSD)

EEMa_returns = as.matrix(na.omit(ClCl(EEMa)))
EEMReturnAvg=mean(EEMa_returns)
EEMReturnSD=sd(EEMa_returns)
eem=c(EEMReturnAvg,EEMReturnSD)

VNQa_returns = as.matrix(na.omit(ClCl(VNQa)))
VNQReturnAvg=mean(VNQa_returns)
VNQReturnSD=sd(VNQa_returns)
vnq=c(VNQReturnAvg,VNQReturnSD)

df<-data.frame(spy,tlt,lqd,eem,vnq)
rownames(df)<-c('Avg Return', 'SD Return')
df
```
As you can see from the above dataframe, the Emerging-market equities has the highest expected return, but also the highest standard deviation. Therefore, EEM is the riskiest of the above ETFs. The Investment-grade corporate bonds have the lowest average return and also the lowest standard deviation. These facts make LQD the safest choice as an investment, but you have a smaller chance of making a larger return. US domestic equities and US Treasury bonds (SPY and TLT) are also relatively safe investments, with low risk but relatively low returns. Real estate (VNQ) has the second highest average return and the second highest standard deviation as well, making it safer than EEM but riskier than the rest. 


2. Outline our choice of safe and aggressive portfolios. 

For our safe portfolio, we decided to only invest in Investment-grade corporate bonds (LQD), US treasury bonds (TLT), and US domestic equities (SPY). These were the three safest investment choices due to their low standard deviation and risk. The expected returns are not as high as the other ETF's, but in the safe portfolio, we minimize the risk in the risk-reward tradeoff. We invested 60% in LQD (lowest risk), 30% in TLT, and 10% in SPY.
For our aggressive portfolio, we decided to only invest in the Emerging-market equities (EEM) and Real estate (VNQ). These have the highest expected return and the highest standard deviation/risk. In this portfolio, we are increases the risk to hopefully increase the reward. We invested 80% in EEM (the riskiest) and 20% in VNQ.


3. Use bootstrap resampling to estimate the 4-week (20 trading day) value at risk of each of your three portfolios at the 5% level.

```{r}
#####################################################
#equal split
#####################################################
all_returns1 = cbind(	ClCl(SPYa),
                     ClCl(TLTa),
                     ClCl(LQDa),
                     ClCl(EEMa),
                     ClCl(VNQa))
all_returns1 = as.matrix(na.omit(all_returns1))
head(all_returns1)

# Now simulate many different possible scenarios  
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns1, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today #does this redistribute?
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    holdings = weights * total_wealth 
  }
  wealthtracker
}
# Calculate 5% value at risk
equalVAR=quantile(sim1[,n_days], 0.05) - initial_wealth

################################################################
#something safer
################################################################

all_returns2 = cbind(	ClCl(SPYa),
                      ClCl(TLTa),
                      ClCl(LQDa))
all_returns2 = as.matrix(na.omit(all_returns2))
head(all_returns2)

# Now simulate many different possible scenarios  
initial_wealth = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.1, 0.3, 0.6)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns2, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    holdings = weights * total_wealth 
  }
  wealthtracker
}
# Calculate 5% value at risk
safeVAR=quantile(sim2[,n_days], 0.05) - initial_wealth

################################################################
#something more aggressive
################################################################

all_returns3 = cbind(ClCl(EEMa),
                      ClCl(VNQa))
all_returns3 = as.matrix(na.omit(all_returns3))
head(all_returns3)

# Now simulate many different possible scenarios  
initial_wealth = 100000
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.8,0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns3, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    holdings = weights * total_wealth 
  }
  wealthtracker
}
# Calculate 5% value at risk
aggressiveVAR=quantile(sim3[,n_days], 0.05) - initial_wealth
df2=rbind(equalVAR, safeVAR, aggressiveVAR)
df2

```
In the dataframe labeled df2, you can see the value at risk at the 5% level for the Equal portfolio, Safe portfolio, and Aggressive portfolio, respectively. As you would expect, the value at risk for the safest portfolio is the smallest, and the value at risk for the aggressive portfolio is the largest.

4. Compare the results for each portfolio in a way that would allow the reader to make an intelligent decision among the three options.

```{r}

s1d=sd(sim1[,n_days]) #sd of values at the end of the 20 days
mean1=mean(sim1[,n_days]) #100800.4

equal_confint= range(mean1-2*sd1, mean1+2*sd1)

sd2=sd(sim2[,n_days]) #sd of values at the end of the 20 days
sd2 #2295.886 (smaller than sd1--less range) 
mean2=mean(sim2[,n_days]) #100551 (less than mean1)
safe_confint= range(mean2-2*sd2,mean2+2*sd2)


sd3=sd(sim3[,n_days]) #sd of values at the end of the 20 days
sd3 #14927 (much bigger variation) 
mean3=mean(sim3[,n_days]) #101817 (much more than others)
agg_confint=range(mean3-2*sd3, mean3+2*sd3)

df3<-rbind(equal_confint,safe_confint,agg_confint)
df3

```
In the dataframe above, you can see the 95% confidence intervals for the total wealth one is expected to have after investing $100,000 into the equal, safe, and aggressive portfolios for 20 trading days. Readers will be able to look at this dataframe, see the potential for both loss and gain in each portfolio, and make a decision based on these facts. By computing the expected gain, expected standard deviation, and 95% confidence interval, readers will have a good idea about the risk-reward tradeoff for each portfolio.


## Market Segmentation

```{r}
ms<-read.csv("marketsegmentation.csv")



